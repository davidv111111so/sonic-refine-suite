"""
AI Mastering Backend
Flask application for processing audio files with GCS integration and Matchering
"""
import os
import json
import time
import uuid
from datetime import timedelta
from flask import Flask, request, jsonify
from flask_cors import CORS
from google.cloud import storage
import tempfile
import matchering as mg

app = Flask(__name__)

# Configure CORS - Allow all origins in development, specific origins in production
ALLOWED_ORIGINS_ENV = os.getenv('ALLOWED_ORIGINS', '*')
if ALLOWED_ORIGINS_ENV == '*':
    ALLOWED_ORIGINS = ['*']
else:
    ALLOWED_ORIGINS = ALLOWED_ORIGINS_ENV.split(',')

CORS(app, resources={
    r"/api/*": {
        "origins": ALLOWED_ORIGINS,
        "methods": ["GET", "POST", "PUT", "DELETE", "OPTIONS", "HEAD"],
        "allow_headers": ["Content-Type", "Authorization", "x-goog-resumable"],
        "expose_headers": ["Content-Type", "Content-Length"],
        "supports_credentials": True
    },
    r"/health": {
        "origins": ["*"],
        "methods": ["GET", "OPTIONS"]
    }
})

# GCS Configuration
PROJECT_ID = os.getenv('GOOGLE_CLOUD_PROJECT_ID')
BUCKET_NAME = os.getenv('GOOGLE_CLOUD_BUCKET_NAME')

# Initialize GCS client
def get_storage_client():
    """Initialize Google Cloud Storage client with credentials from environment"""
    credentials_json = os.getenv('GOOGLE_APPLICATION_CREDENTIALS_JSON')
    if not credentials_json:
        raise ValueError("GOOGLE_APPLICATION_CREDENTIALS_JSON not set")
    
    # Parse JSON credentials
    credentials_dict = json.loads(credentials_json)
    
    # Create credentials from dict
    from google.oauth2 import service_account
    credentials = service_account.Credentials.from_service_account_info(credentials_dict)
    
    return storage.Client(credentials=credentials, project=PROJECT_ID)

@app.route('/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    return jsonify({"status": "OK", "service": "AI Mastering Backend"}), 200

@app.route('/api/master-audio', methods=['POST', 'OPTIONS'])
def master_audio():
    """
    Process audio file with Matchering mastering
    
    Request body:
    {
        "targetUrl": "https://storage.googleapis.com/.../target.wav",
        "referenceUrl": "https://storage.googleapis.com/.../reference.wav",
        "fileName": "input.wav",
        "settings": {
            "threshold": 0.998138,
            "fft_size": 4096,
            ...
        }
    }
    
    Response:
    {
        "success": true,
        "masteredUrl": "https://storage.googleapis.com/.../output.wav",
        "jobId": "uuid",
        "processingTime": 1234
    }
    """
    # Handle CORS preflight
    if request.method == 'OPTIONS':
        return '', 204
    
    start_time = time.time()
    
    try:
        # Parse request
        data = request.get_json()
        if not data:
            return jsonify({"error": "No JSON data provided"}), 400
        
        target_url = data.get('targetUrl')
        reference_url = data.get('referenceUrl')
        file_name = data.get('fileName')
        settings = data.get('settings', {})
        
        if not target_url or not reference_url or not file_name:
            return jsonify({"error": "Missing required fields: targetUrl, referenceUrl, fileName"}), 400
        
        # Generate job ID
        job_id = str(uuid.uuid4())
        
        # Initialize GCS client
        storage_client = get_storage_client()
        bucket = storage_client.bucket(BUCKET_NAME)
        
        # Download TARGET file from GCS
        print(f"ðŸ“¥ Downloading TARGET from: {target_url}")
        target_blob_name = extract_blob_name_from_url(target_url)
        target_blob = bucket.blob(target_blob_name)
        
        with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as temp_target:
            target_blob.download_to_filename(temp_target.name)
            target_path = temp_target.name
        
        print(f"âœ… TARGET downloaded to: {target_path}")
        
        # Download REFERENCE file from GCS
        print(f"ðŸ“¥ Downloading REFERENCE from: {reference_url}")
        reference_blob_name = extract_blob_name_from_url(reference_url)
        reference_blob = bucket.blob(reference_blob_name)
        
        with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as temp_reference:
            reference_blob.download_to_filename(temp_reference.name)
            reference_path = temp_reference.name
        
        print(f"âœ… REFERENCE downloaded to: {reference_path}")
        
        # Create temp output file
        with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as temp_output:
            output_path = temp_output.name
        
        # Process audio with REAL Matchering
        print(f"ðŸŽµ Starting Matchering processing...")
        print(f"Settings: {settings}")
        
        try:
            # Configure Matchering output bit depth
            output_bits = settings.get('output_bits', 16)
            if output_bits == 16:
                result_format = mg.pcm16(output_path)
            elif output_bits == 24:
                result_format = mg.pcm24(output_path)
            else:
                result_format = mg.pcm32(output_path)
            
            # Run Matchering with settings
            mg.process(
                target=target_path,
                reference=reference_path,
                results=[result_format],
                # Matchering settings
                threshold=settings.get('threshold', 0.998138),
                max_iterations=settings.get('max_iterations', 50),
                max_piece_size=settings.get('max_piece_length', 30.0),
                internal_sample_rate=48000
            )
            
            print(f"âœ… Matchering processing complete!")
            
        except Exception as matchering_error:
            print(f"âŒ Matchering error: {str(matchering_error)}")
            raise Exception(f"Matchering processing failed: {str(matchering_error)}")
        
        # Upload result to GCS
        output_file_name = f"mastered/{job_id}/{file_name}"
        output_blob = bucket.blob(output_file_name)
        
        print(f"ðŸ“¤ Uploading to GCS: {output_file_name}")
        try:
            output_blob.upload_from_filename(output_path)
            print(f"âœ… File uploaded to GCS successfully")
        except Exception as upload_error:
            print(f"âŒ Error uploading to GCS: {str(upload_error)}")
            raise Exception(f"Failed to upload mastered file to GCS: {str(upload_error)}")
        
        # Generate signed URL for download
        try:
            mastered_url = output_blob.generate_signed_url(
                version="v4",
                expiration=timedelta(hours=1),  # 1 hour
                method="GET"
            )
            print(f"âœ… Signed URL generated: {mastered_url[:80]}...")
        except Exception as url_error:
            print(f"âŒ Error generating signed URL: {str(url_error)}")
            # Fallback: use public URL if bucket is public, or return error
            raise Exception(f"Failed to generate download URL: {str(url_error)}")
        
        # Cleanup temp files
        try:
            if os.path.exists(target_path):
                os.unlink(target_path)
            if os.path.exists(reference_path):
                os.unlink(reference_path)
            if os.path.exists(output_path):
                os.unlink(output_path)
        except Exception as cleanup_error:
            print(f"âš ï¸ Warning: Error cleaning up temp files: {str(cleanup_error)}")
        
        processing_time = int((time.time() - start_time) * 1000)
        
        return jsonify({
            "success": True,
            "masteredUrl": mastered_url,
            "jobId": job_id,
            "processingTime": processing_time
        }), 200
        
    except Exception as e:
        print(f"âŒ Error processing audio: {str(e)}")
        processing_time = int((time.time() - start_time) * 1000)
        return jsonify({
            "success": False,
            "error": str(e),
            "processingTime": processing_time
        }), 500

def extract_blob_name_from_url(url):
    """Extract blob name from GCS URL"""
    # Expected format: https://storage.googleapis.com/bucket-name/blob/path
    # Or signed URL format
    
    # Handle test URLs gracefully
    if 'test.url' in url or 'example.com' in url:
        # Extract filename from test URL
        return url.split('/')[-1]
    
    if '?' in url:
        # Signed URL - extract path before query params
        url = url.split('?')[0]
    
    # Remove protocol and domain
    if 'storage.googleapis.com' in url:
        parts = url.split('storage.googleapis.com/')
        if len(parts) > 1:
            path = parts[1]
            # Remove bucket name (first part after domain)
            path_parts = path.split('/', 1)
            if len(path_parts) > 1:
                return path_parts[1]
            # If there's only one part, it might be just the file name in a different bucket
            # For test purposes, return the filename
            return path_parts[0].split('/')[-1]
    
    # If URL format is different, try to extract from alternative formats
    if BUCKET_NAME and BUCKET_NAME in url:
        bucket_index = url.find(BUCKET_NAME)
        path = url[bucket_index + len(BUCKET_NAME):].lstrip('/')
        if '?' in path:
            path = path.split('?')[0]
        return path
    
    # Last resort: try to extract any path after googleapis.com
    if 'googleapis.com' in url:
        parts = url.split('googleapis.com/')
        if len(parts) > 1:
            # Get everything after the domain, remove query params
            path = parts[1].split('?')[0]
            # If it has slashes, assume format is bucket/path, return path
            if '/' in path:
                return '/'.join(path.split('/')[1:])
            return path
    
    raise ValueError(f"Could not extract blob name from URL: {url}")

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 8080))
    app.run(host='0.0.0.0', port=port)
